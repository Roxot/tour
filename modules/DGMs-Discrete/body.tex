\begin{frame}{Generative Models}
Joint distribution over observed data $ x $ and latent variables $ z $.
\begin{equation*}
p(x,z|\theta) =  \underbrace{p(z)}_{\text{prior}} \underbrace{p(x|z,\theta)}_{\text{likelihood}}
\end{equation*} 
The likelihood and prior are often standard distributions (Gaussian, Bernoulli) with simple dependence on conditioning
information.
\end{frame}

\begin{frame}{Deep generative models}

Joint distribution with {\bf deep observation model}
\begin{equation*}
p(x, z|\theta) = \underbrace{p(z)}_{\text{prior}} \underbrace{p(x|z, \theta)}_{\text{likelihood}}
\end{equation*}
~ {\small mapping from $z$ to $p(x|z, \theta)$ is a neural network with parameters $\theta$}

~ \pause

Marginal likelihood 
\begin{equation*}
p(x|\theta) = \int p(x, z|\theta) \dd{z} = \int p(z)p(x|z, \theta) \dd{z} 
\end{equation*}
~ \alert{intractable} in general



\end{frame}


\begin{frame}{Goals}


We want
\begin{itemize}
	\item richer probabilistic models  \pause
	\item complex observation models \\
	parameterised by neural networks 
	%\item but we cannot use backprop for parameter estimation \\
	%due to intractability of log-marginal and its gradient
\end{itemize}
\pause

~

but we can't perform gradient-based MLE

~

\pause

We need \alert{approximate inference} techniques!

\end{frame}


\begin{frame}{ELBO recap}

And we've developed the ELBO
\begin{equation*}
\begin{aligned}
\log p(x) &= \log \int p(z, x) \dd z ~ \pause = \log \int q(z|x) \frac{p(z, x)}{q(z|x)} \dd z \\ \pause
&\overset{\text{JI}}{\ge} \underbrace{\E[q(z|x)]{\log \frac{p(z, x)}{q(z|x)}} }_{\ELBO} %\\ &= \E[q(z|x)]{\log p(x, z)} + \Ent{q(z|x)}
& \\ \pause
\ELBO &= \E[q(z|x)]{\log \frac{p(z|x)p(x)}{q(z|x)}} \\ \pause
%&=\E[q(z|x)]{\log \frac{p(z|x)}{q(z|x)}} + \log p(x) \\
&=-\underbrace{\KL{q(z|x)}{p(z|x)}}_{\text{gap}} + \log p(x)
\end{aligned}
\end{equation*}

\end{frame}


\section{First Attempt: Wake-Sleep}
\frame{\tableofcontents[currentsection]}


\input{WS}

\section{Neural Variational Inference and Learning}
\frame{\tableofcontents[currentsection]}

\input{NVIL}

\begin{frame}{Summary}
\begin{itemize}
\item Wake-Sleep: train inference and generation networks with separate objectives
\pause
\item NVIL: a single objective (ELBO) for both models\\ \pause
\item Use score function estimator\\ 
\pause
\item Always use baselines for variance reduction!
\end{itemize}
\end{frame}

\begin{frame}{Implementation}

Check one of our notebooks, e.g.
\begin{itemize}
	\item inducing rationales for sentiment classification \\
\url{github.com/vitutorial/exercises/tree/master/SST}
\end{itemize}


\end{frame}

\begin{frame}[allowframebreaks]{Literature}
%\nocite{KingmaWelling:2013}
\nocite{HintonEtAl:1995}
\nocite{MnihNVIL}
 \nocite{PaisleyEtAl:2012}
 \nocite{RanganathEtAl:2014}
 \nocite{Williams:1992}
 \nocite{greensmith2004variance} 
 \nocite{gu2015muprop}
 \nocite{tucker2017rebar}
 \nocite{grathwohl2017backpropagation}



\bibliographystyle{unsrtnat}
\bibliography{../../VI}
\end{frame}
